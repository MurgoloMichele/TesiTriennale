\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Introduzione}

La continua digitalizzazione nel mondo sta mettendo le aziende a rischio di attacchi informatici più che mai. Negli ultimi anni, grazie alla crescente adozione di servizi cloud e mobili, la sicurezza delle informazioni ha subito un profondo cambio di paradigma, influenzando sia i tradizionali strumenti di protezione (come antivirus e firewall), sia le piattaforme dedicate all'individuazione di attività dannose all'interno delle reti aziendali~\cite{Symantec:Report18}.
I metodi di attacco sempre più sofisticati utilizzati dai criminali informatici in diverse recenti violazioni della sicurezza su larga scala mettono chiaramente in mostra l'inadeguatezza dei tradizionali approcci per la sicurezza delle informazioni ~\cite{Ukrainian:Report, WikiLeaks:Vault7}.
Con attacchi sempre più avanzati e persistenti e il semplice fatto che ogni organizzazione deve proteggersi da molteplici varietà di attacchi mentre un aggressore ha bisogno solo di un tentativo riuscito per compromettere un'intera azienda, è necessario che le organizzazioni rivalutino, modifichino e aggiornino i propri sistemi di difesa. Lo stato dell'arte mette in evidenza come l'analisi dei dati costituisca un elemento chiave per migliorare drasticamente le performance dei meccanismi difensivi, che consente un aumento sensibile della rilevazione di azioni malevole \cite{bigdata}.

\textit{Big data security analytics} è l'approccio alla base di questo miglioramento del rilevamento. Il rilevamento deve essere in grado di identificare i modelli di utilizzo in continuo cambiamento, ed eseguire analisi complesse su una varietà di fonti di dati che vanno dai registri di server e applicazioni agli eventi di rete e alle attività degli utenti.
Ciò richiede di eseguire analisi su grandi quantità di dati correnti e storici.

Negli ultimi anni è emersa una nuova generazione di soluzioni di analisi della sicurezza, in grado di raccogliere, archiviare e analizzare enormi quantità di dati~\cite{IBM:Watson, Darktrace:ML}. Questi dati vengono analizzati applicando vari algoritmi di correlazione per rilevare le anomalie e quindi identificare possibili attività dannose.
L'industria ha finalmente raggiunto il punto in cui gli algoritmi di intelligenza artificiale per l'elaborazione di dati su larga scala sono diventati accessibili utilizzando framework prontamente disponibili~\cite{ApacheSpot:URL}.
Ciò consente di combinare analisi storiche e in tempo reale e identificare nuovi incidenti che potrebbero essere correlati ad altri che si sono verificati in passato. Insieme a fonti di intelligence di sicurezza esterne~\cite{Virustotal:URL} che forniscono informazioni aggiornate sulle ultime vulnerabilità, ciò può facilitare notevolmente l'identificazione di attacchi informatici in corso sulla rete~\cite{Apruzzese:Periodic}.

È con l'intenzione di utilizzare queste tecnologie che viene presentato in questa tesi una suite di software per la rilevazione di intrusioni nella rete, che fanno uso dei più recenti meccanismi per i processi di cattura e analisi di enormi quantità di dati. Questa tesi metterà in particolare evidenza le difficoltà nella gestione delle diverse tipologie di formati richieste da queste tecnologie. Verranno sottolineate l'eterogeneità dei software che hanno sempre contraddistinto l'informatica e le soluzioni scelte per risolvere tali problemi. A questo scopo, è stato creato un algoritmo per la conversione del traffico di rete tra due formati differenti. Tale algoritmo ha richiesto la ricerca e lo sviluppo di soluzioni originali per il raggiungimento di un duplice obiettivo: (i) la preservazione dei dati durante le conversioni; (ii) l'ottenimento di una elevata efficienza in termini di velocità di esecuzione del programma. Infatti, la produzione di un algoritmo di conversione con perdita di dati (\textit{lossy}) porterebbe inevitabilmente ad una riduzione delle performance di rilevazione. Allo stesso modo, la necessità di effettuare analisi in tempo reale (al fine di rilevare attacchi il prima possibile) richiede tempistiche di esecuzione il più rapide possibili. Per lo sviluppo della soluzione proposta, si è pertanto fatto ricorso ad un approccio di calcolo parallelo per sfruttare la potenza dei più recenti processori multi core, velocizzando notevolmente i tempi di analisi. Il programma prodotto è stato sottoposto ad un'ampia serie di esperimenti, volti a misurarne le performance di esecuzione. Tali esperimenti dimostrano che la parallelizzazione nell'esecuzione raggiunge uno speedup lineare grazie al quale è possibile convertire migliaia di files in pochi secondi.
Infine, al fine di facilitare l'utilizzo del software analizzato, verranno anche spiegate in dettaglio tutte le procedure e operazioni necessarie per il corretto deployment della piattaforma di \textit{Intrusion Detection System} (IDS) analizzata.

La parte rimanente di questa tesi è strutturata come segue.

Nel secondo capitolo verranno in primo luogo presentate le minacce provenienti dalla rete, con particolare enfasi sui tipi di attacchi su larga scala. Successivamente verrà presentato lo stato dell'arte dei sistemi di difesa utilizzati ad oggi per contrastare questi tipi di attacchi. In conclusione sarà fatta una introduzione al machine learning usato da questi sistemi.

Nel terzo capitolo verrà introdotto uno speciale tipo di file che sarà il principale punto di enfasi di questa tesi. Quindi verranno presentati i differenti tipi di formati che questo file può assumere e le problematiche dovute ai diversi standard in uso oggi. Dopo una descrizione dettagliata delle varie differenze tra i formati verrà introdotto il software che farà uso di questi file e le problematiche legate all'utilizzo di software avanzati ma ancora in fase di alpha. Si presenterà poi l'utilizzo che questi file hanno in relazione alle tecnologie utilizzate. Infine troverà spazio la presentazione del problema da affrontare.

Nel quarto capitolo si descriveranno le scelte effettuate per risolvere il problema del deployment di un software non ancora pubblicato e come si è modificato il codice sorgente per poterlo adattare alle proprie esigenze. Successivamente verrà analizzato l'uso di diversi formati di file e l'automatizzazione di tale soluzione. Infine saranno presentati i diversi metodi atti a perfezionare l'automatizzazione per renderla efficiente e la scelta che è stata effettuata.

Nel quinto capitolo verranno mostrati i risultati dei test e i benchmark effettuati con lo scopo di confermare al livello pratico quanto mostrato sotto forma teorica nel capitolo precedente. Saranno descritte in modo dettagliato le condizioni sotto le quali sono stati effettuati i test e limiti della scalabilità della soluzione. Il tutto verrà seguito da grafici esplicativi.

Nel sesto capitolo infine, verranno presentate le conclusioni e possibili strade per lavoro futuro.
