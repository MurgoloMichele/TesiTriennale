\documentclass[../main.tex]{subfiles}
\begin{document}
\chapter{Esperimenti e risultati}

In questo capitolo vengono presentati gli esperimenti effettuati e le prestazioni ottenuti dallo script di conversione, successivamente i test fatti con Stratosphere IPS sulla creazione di modelli comportamentali per verificare l'accuratezza della conversione dei flows.

\section{Software utilizzati}

Per lo sviluppo del programma per la conversione dei file e per l'utilizzo di Stratosphere IPS è stata creata una piattaforma dedicata alla Security Analytics tramite l'utilizzo di \textit{VirtualBox} con una installazione del sistema operativo Ubuntu 16.04 LTS.


\begin{verse}
				\textbf{VirtualBox} è un software gratuito e open source per l'esecuzione di macchine virtuali che supporta Windows, GNU/Linux e macOS come sistemi operativi host ed è in grado di eseguire Windows, GNU/Linux, OS/2 Warp, BSD come ad esempio OpenBSD, FreeBSD e infine Solaris e OpenSolaris come sistemi operativi guest \cite{virtualbox}. 
\end{verse}

Per controllare se la conversione viene effettuata correttamente si è usato il software Stratosphere IPS per creare e utilizzare dei modelli impiegati per fare \textit{anomaly detection} su del traffico di rete. I flows sono stati convertiti nel formato compatibile da Argus ad nProbe e infine di nuovo ad Argus per controllare che non ci siano delle perdite di informazioni vitali che alterino il funzionamento dell'IDPS.

La figura seguente descrive il processo di conversione. Ad ogni conversione effettuata è prevista una perdita di informazioni dovuta al tipo di campi utilizzati nei diversi formati e alla loro formattazione.

\begin{figure}[H]
				\centering
				\includegraphics[scale=0.5]{conversion.png}
				\caption{Processo di conversione}
\end{figure}

\section{Installazione Stratosphere IPS}
Sulla macchina virtuale è stato installato il framework di Stratospehere IPS. Di seguito i passaggi effettuati per l'installazione \cite{stf}.

\begin{itemize}
				\item Installazione del programma git 2.7.4
\begin{lstlisting}[language=bash]
$ sudo apt install git
\end{lstlisting}

				\item Clonazione repository github del framework
\begin{lstlisting}[language=bash]
$ git clone https://github.com/stratosphereips/StratosphereTestingFramework
\end{lstlisting}

				\item Installazione del programma python-pip
\begin{lstlisting}[language=bash]
$ sudo apt install python-pip
\end{lstlisting}
\end{itemize}
				
\begin{itemize}
				\paragraph{Installazione dipendenze per Stratosphere IPS}

				\item prettytable 0.7.2-3
\begin{lstlisting}[language=bash]
$ sudo apt install python-prettytable
\end{lstlisting}

				\item transaction 1.4.3-3
\begin{lstlisting}[language=bash]
$ sudo apt install python-transaction
\end{lstlisting}

				\item persistent 4.1.1-1build2
\begin{lstlisting}[language=bash]
$ sudo apt install python-persistent
\end{lstlisting}

				\item zodb 5.4.0
\begin{lstlisting}[language=bash]
$ sudo pip install zodb
\end{lstlisting}

				\item sparse 1.1-1.3build1
\begin{lstlisting}[language=bash]
$ sudo apt install python-sparse
\end{lstlisting}

				\item dateutil 2.4.2-1
\begin{lstlisting}[language=bash]
$ sudo apt install python-dateutil
\end{lstlisting}


\end{itemize}

\section{Installazione di Argus}

Per generare i netflow dal traffico di rete è necessario avere una istanza di Argus sul computer. I seguenti passaggi sono necessari per la corretta installazione di Argus \cite{stf}.

\begin{itemize}
				\item libpcap 1.7.4-2
\begin{lstlisting}[language=bash]
$ sudo apt install libpcab-dev
\end{lstlisting}

\item bison 3.0.4
\begin{lstlisting}[language=bash]
$ sudo apt install bison
\end{lstlisting}

\item flex 2.6.0-11
\begin{lstlisting}[language=bash]
$ sudo apt install flex
\end{lstlisting}

\item Installazione dell'ultima versione di argus 3.0.8.2 dal sito http://qosient.com/argus/dev/argus-latest.tar.gz

\item Installazione dell'ultima versione di argus-client 3.0.8.2 dal sito http://qosient.com/argus/dev/argus-clients-latest.tar.gz

\end{itemize}
\section{Utilizzo del programma stf\\}
*!*!*!*!*!*!*!*!* Questa parte mi sembra brutta, per ogni riga dovrei caricare un'immagine presa da github, la caption è direttamente la riga sopra !*!*!*!*!*!*!*\newline

Per eseguire il programma lo si esegue con
\begin{lstlisting}[language=bash]
	./stf.py
\end{lstlisting}
\includegraphics[scale=0.5]{homestf.png}

Per caricare un dataset si utilizza il comando
\begin{lstlisting}[language=bash]
	datasets -c /absolute/path/file.binetflow	
\end{lstlisting}

Per generare la connessione si utilizza il comando
\begin{lstlisting}[language=bash]
	connections -g	
\end{lstlisting}

Infine, per generare i modelli, il comando
\begin{lstlisting}[language=bash]
	models -g	
\end{lstlisting}

Per visualizzare il behavioral model si utilizza il comando
\begin{lstlisting}[language=bash]
	models -L [id]	
\end{lstlisting}
\includegraphics[scale=0.5]{stfmodels.png}
\end{document}

\section{Prestazioni}

Come visto nel capitolo 4, il programma di conversione proposto è efficiente. In questa sezione vengono effettuati dei \textit{benchmark} per analizzarne le prestazioni e studiare la scalabilità di una soluzione che prevede l'utilizzo di più CPU in parallelo.

\subsection{Benchmark}

Per misurare la velocità del programma si è usato come dataset 10 giorni di flows. La dimensione del dataset è di 1,7 Gb per un totale di 14400 file. Le misure sono state realizzate con il programma /usr/bin/time.

Il computer su cui sono stati effettuati i benchmark ha le seguenti caratteristiche:
\begin{itemize}
				\item Linux Mint 19 Cinnamon
				\item Kernel 4.15.0-20-generic
				\item Processore Intel Core i7-3770 @ 3.40GHz 
				\item RAM 8 Gb
				\item HDD 2 Tb
				\item AMD HD7870
\end{itemize}

È stato effettuato un benchmark per ogni core disponibile sulla CPU, a seguire i risultati:
\begin{itemize}
				\item 1 core: 11.35.46
				\item 2 core: 6.02.65
				\item 3 core: 4.12.62
				\item 4 core: 3.09.79
				\item 5 core: 3.04.47
				\item 6 core: 3.04.66
				\item 7 core: 2.59.22
				\item 8 core: 2.59.33
\end{itemize}

La CPU su cui sono stati effettuati i benchmark ha 4 core fisici e 4 virtuali, si ha quindi un guadagno lineare per i primi 4 core, dopodichè il guadagno non è più lineare.

L'immagine seguente ha sull'asse delle $x$ il numero di core e sull'asse delle $y$ il tempo impiegato per convertire 14400 file. Come si può vedere il tempo impiegato scende in modo quasi lineare per i primi quattro core.

\begin{figure}[H]
				\centering
				\includegraphics[scale=0.7]{graph1.png}
				\caption{Tempo impiegato per la conversione rispetto al numero di core}
\end{figure}

\subsection{Metrica delle prestazioni parallele}

Sia $T$(p) il tempo di esecuzione in secondi di un certo algoritmo su $p$ processori. Di conseguenza sia $T$(1) il tempo di esecuzione del codice parallelo su 1 processore.
La \textit{misura di scalabilità} o \textit{speedup} relativo di un algoritmo parallelo eseguito su \textit{p} processori si calcola come:

\begin{center}
\begin{math}
S(p) = \frac{T(1)}{T(p)}
\end{math}
\end{center}

In un sistema ideale, in cui il carico di lavoro potrebbe essere perfettamente partizionato su $p$ processori, lo speedup relativo dovrebbe essere uguale a $p$. In questo caso si parla di speedup lineare.

Con i risultati ottenuti in precedenza si è calcolato lo speedup per ogni numero di core. La figura seguente mostra lo speedup relativo per numero di core. È possibile notare come lo speedup sia lineare per i primi quattro core e quindi ottimale, mentre aggiungendo più core si ha comunque un guadagno ma minimo.

!*!*!*! Devo mostrare le formule per ogni numero o lascio solo il grafico !*!*!*!

\begin{figure}[H]
				\centering
				\includegraphics[scale=0.7]{graph2.png}
				\caption{Speedup relativo per ogni core}
\end{figure}

Si definisce \textit{efficienza} il rapporto
\begin{center}
\begin{math}
E(p) = \frac{S(p)}{p}
\end{math}
\end{center}

Idealmente, se l'algoritmo avesse uno speedup lineare, si avrebbe 
\begin{math}
E(p) = 1
\end{math}

Più l'efficienza si allontana da 1, peggio stiamo sfruttando le risorse di calcolo disponibili nel sistema parallelo.

La figura seguente mostra l'efficienza del programma di conversione. Usando quattro core si ha un'efficienza leggermente maggiore di 0.9 mentre con l'aggiunta di altri core in questo grafico si può notare chiaramente come l'efficienza cali.
\begin{figure}[H]
				\centering
				\includegraphics[scale=0.7]{graph3.png}
				\caption{Efficienza per numero di core}
\end{figure}

La conversione in parallelo risulta molto efficiente, intorno allo 0.9. Con gli esperimenti effettuati ci si aspetta uno speedup lineare sui core fisici.
Utilizzando un solo core la conversione impiega 12 minuti, mentre con quattro core si divide in quattro il tempo arrivando a 3 minuti. Le CPU di nuova generazione come AMD Ryzen vantano 32 core fisici e 32 virtuali, con questo numero di core fisici ci si può aspettare di dividere il tempo per 32, arrivando ad effettuare la conversione in 23 secondi.

\section{Modelli comportamentali}

\end{document}

